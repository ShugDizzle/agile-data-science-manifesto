# Agile Data Manifesto


## Front Page

Is the Agile Manifesto plus the following

Building a product over being academic

Working software over proofs of concepts

Computational methods over statistical tricks

Automation over interactive analysis

Clear evaluation over complex models

That is, while there is value in the items on
the right, we value the items on the left more.

## Twelve Principles

Our highest priority is to satisfy the customer
through early and continuous delivery of data driven
insights and actions that are entirely automated and
independent of the creator.

Data Science without automation and software principles is
not Data Science, it is a glorification of traditional Analytics with
open source technology.

Deliver automated data driven insights frequently, from a
week to a couple of weeks, with a 
preference to the shorter timescale.

A Data Scientist does not exist as an individual, they can only
exist as a member of a Data Science team that includes a wide set
of skills including Engineers, Developers, DevOps and Theorists.

All members of a Data Science team should
provide input to all areas of development and over
time learn all the skills to some degree.

Working software is the primary measure of progress.
Even in Data Science. Documentation, presentations and notebooks
come second.

Prototypes and "Proofs of Concept" should be avoided or
easily modifiable into production ready solutions.  Time should
not be wasted rewriting the same solution in different languages.

Evaluation of a model must be designed prior to building a model, they should be robust and high quality modules.  Without such a process we have Data Art.

The proven practices of Software Development are
applicable to Data Science, like Continuous Integration and
Test Driven Development. Rejection of this is either elitism, snobbery or laziness.

Simple models that clearly represent the business use case tend
to perform as well, and if not better than, complex models. The meaning and evaluation of the model should be in business terms over academic terms.

Use the use case and simple business numbers like profit and savings to evaluate models
over obscure statistics.

Avoid assuming you have a can opener, use the data to derive your insights
and use computational power to solve the difficulties rather than statistical tricks.

## DATA SCIENCE
- Knowledge discovery
- Knowledge delivery

We harvest data to produce knowledge. Data analytics means you are clueless. You roam in a vast unknown space. Data science delivers knowledge. In data science we know what we understand, we apply scientific methodology, numerical methods and computer science algorithms to explore the unknown. Our work is not mystical because we document our conclusions, decisions and deductions. 
We are data scientists!

### Knowledge Discovery
From Wiki: Science is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.

From Wiki: The scientific method is a body of techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. To be termed scientific, a method of inquiry is commonly based on empirical or measurable evidence subject to specific principles of reasoning.
The data part of the "data science" is the reflection of the universe through the lens of the measurement instruments used.
It follows that in data science we build and organize knowledge that is being derived from the universe we are investigating. We build testable, reproducible experiments that try to explain interactions within our universe and then make predictions.

The first step in every scientific exploration is the knowledge discovery. Most likely this step is referred by the term data analysis. Data analysis means you are clueless, have no goal and no methodology. We are scientists. We incrementally discover units of knowledge which can be reused to state general or individual properties, build up context, infer and prove structural properties about our universe. These knowledge atoms allow us to infer and predict.

When we start knowledge exploration we know nothing. We have many questions that require answer to reach our goal. State your goal(s). State the questions you have. List the properties you want to observe. State your expectations. State your hypothesis. State your expectations. 

Now use data to answer them. Statistical and numerical methods are means which give us answers to the questions we have stated. We iteratively answer questions from our list using the data. Once evidence accumulates we can make approximations and state conclusions. Write these bits of knowledge down and their supporting evidence. Does it make sense? There is always one more question we can answer to increase the confidence of our finding. Everything needs to be supported by evidence. At any given time we need to know what we know, what we don't know and why is it so.

This is science. Your results will be challenged so prepare your work for scrutiny. Make sure it is reproducible, repeatable.

All data models, inferred results and predictions are produced by numerical models that make approximations, assumptions and have theoretical requirements. The decisions of using or applying any method needs have strong arguments supported by the already accumulated knowledge. Any decision, conclusion, deduction needs to be documented, proved or tested. Your conclusion shouldn't be a surprise but a natural step of your chain of reasoning.

A skilled data scientist knows how to make the data do the talking.

Any person that touches a data science piece of work must be able to understand the chain of conclusions, decisions and deductions which lead to the final results. Equally important the ability to tell why the produced results are correct should be in the hands of everyone.

### Knowledge Delivery
 Use what we have learned in the Knowledge Discovery to build models and ship numbers (inferred, predicted, computed). 
 Here is where software comes in.
 Touch on :
 - data sanity check
 - automatic assumptions check
 - ETL
 - scalable computation
 - agile principle
 - functional programming benefits.
 - 
 
### In data science the feedback loop contains the two stages...discovery, delivery <- repeat. 
Discover one more knowledge bit, incorporate in software, ship, repeat. TO BE CONTINUED...
