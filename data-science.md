---
layout: page
title: Data Science
permalink: /data-science
---

Data Science can be split into:

* Knowledge discovery
* Knowledge delivery

We harvest data to produce knowledge. Data analytics means you are clueless. You roam in a vast unknown space. Data science delivers knowledge. In data science we know what we understand, we apply scientific methodology, numerical methods and computer science algorithms to explore the unknown. Our work is not mystical because we document our conclusions, decisions and deductions. 
We are data scientists!

### Knowledge Discovery
From Wiki: Science is a systematic enterprise that builds and organizes knowledge in the form of testable explanations and predictions about the universe.

From Wiki: The scientific method is a body of techniques for investigating phenomena, acquiring new knowledge, or correcting and integrating previous knowledge. To be termed scientific, a method of inquiry is commonly based on empirical or measurable evidence subject to specific principles of reasoning.
The data part of the "data science" is the reflection of the universe through the lens of the measurement instruments used.
It follows that in data science we build and organize knowledge that is being derived from the universe we are investigating. We build testable, reproducible experiments that try to explain interactions within our universe and then make predictions.

The first step in every scientific exploration is the knowledge discovery. Most likely this step is referred by the term data analysis. Data analysis means you are clueless, have no goal and no methodology. We are scientists. We incrementally discover units of knowledge which can be reused to state general or individual properties, build up context, infer and prove structural properties about our universe. These knowledge atoms allow us to infer and predict.

When we start knowledge exploration we know nothing. We have many questions that require answer to reach our goal. State your goal(s). State the questions you have. List the properties you want to observe. State your expectations. State your hypothesis. State your expectations. 

Now use data to answer them. Statistical and numerical methods are means which give us answers to the questions we have stated. We iteratively answer questions from our list using the data. Once evidence accumulates we can make approximations and state conclusions. Write these bits of knowledge down and their supporting evidence. Does it make sense? There is always one more question we can answer to increase the confidence of our finding. Everything needs to be supported by evidence. At any given time we need to know what we know, what we don't know and why is it so.

This is science. Your results will be challenged so prepare your work for scrutiny. Make sure it is reproducible, repeatable.

All data models, inferred results and predictions are produced by numerical models that make approximations, assumptions and have theoretical requirements. The decisions of using or applying any method needs have strong arguments supported by the already accumulated knowledge. Any decision, conclusion, deduction needs to be documented, proved or tested. Your conclusion shouldn't be a surprise but a natural step of your chain of reasoning.

A skilled data scientist knows how to make the data do the talking.

Any person that touches a data science piece of work must be able to understand the chain of conclusions, decisions and deductions which lead to the final results. Equally important the ability to tell why the produced results are correct should be in the hands of everyone.

### Knowledge Delivery

Knowledge discovery is the step where we understand the laws that govern the universe in which we have to solve our problem. Our problem has been stated with respect to a set of requirements and properties of the dataset.
Knowledge delivery is where we use the the math, statistics and computer science to build a software mechanism that deliver the goal we have set initially while making sure all of our assumptions, hypothesis and conditions are continuously fullfilled. The validity of the pre-conditions for a problem must always be verified before running any numerical algorithm. Once the pre-conditions become invalid, there is a different problem being solved which we didn't aim at. Running the correct algorithm on the correct data is the only way to deliver correct results. 

Numerical algorithms are almost always producing number regardless of the numbers fed into them. There are only two ways to guarantee the delivery of accurate results:
- testing the data
- testing the algorithms.

1. Data testing happens before every run of the algorithm. It checks the validity of the numerical requirements for the algorithm as they have been stated in Knowledge Discovery. 
2. Algorithm testing is performed using unit tests and integration tests, topic treated extensively in software engineering.

If the integrity of your numbers cannot be verified, what you have produced is not science but something else.

 Use what we have learned in the Knowledge Discovery to build models and ship numbers (inferred, predicted, computed). 
 Here is where software comes in.
 Touch on :
 - data sanity check
 - automatic assumptions check
 - ETL
 - scalable computation
 - agile principle
 - functional programming benefits.

#### THE LOOP
In data science the feedback loop cycles through two stages: knowledge discovery and delivery.
Discover one more bit of knowledge, incorporate in software, ship, repeat.
